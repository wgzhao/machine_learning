{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## General information\n",
    "\n",
    "In Don't Overfit! II competition we have a binary classification task. 300 columns, 250 training samples and 79 times more samples in test data! We need to be able to build a model without overfitting.\n",
    "\n",
    "In this kernel I'll write the following things:\n",
    "\n",
    "* EDA on the features and trying to get some insights;\n",
    "* Using permutation importance to select most impactful features;\n",
    "* Comparing various models: bayer classification, linear models, tree based models;\n",
    "* Trying various approaches to feature selection including taking top features from eli5 and shap;\n",
    "* Hyperparameter optimization for models;\n",
    "* Feature generation;\n",
    "* Other things;\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*vuZxFMi5fODz2OEcpG-S1g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content\n",
    "\n",
    "* [1 Data exploration](#de)\n",
    "* [2 Basic modelling](#bm)\n",
    "* [3 ELI5](#eli5)\n",
    "* [3.1 ELI5 and permutation importance](#eli5p)\n",
    "* [4 SHAP](#shap)\n",
    "* [5 Mlextend SequentialFeatureSelector](#mlextend)\n",
    "* [6 Modelling](#model)\n",
    "* [7 Polynomial features](#poly)\n",
    "* [8 Adding statistics](#stats)\n",
    "* [9 Adding distance features](#dist)\n",
    "* [10 Sklearn feature selection](#select)\n",
    "* [11 GLM](#glm)\n",
    "* [12 Selected top_features + statistics](#selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import shap\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c971fbc1a1b8249045b120924058402a036e665"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b40dd26ead2705ebf0058b0aa528c89a18aedbe"
   },
   "source": [
    "<a id=\"de\"></a>\n",
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67a26174603f6a28709b7e0431aa431832ae608d"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "504b609f61494c21f8b078182e06191581ecb2a6"
   },
   "outputs": [],
   "source": [
    "train[train.columns[2:]].std().plot('hist');\n",
    "plt.title('Distribution of stds of all columns');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7b0f65d2d5c999a44a3f71e013b1b6a6ff08980"
   },
   "outputs": [],
   "source": [
    "train[train.columns[2:]].mean().plot('hist');\n",
    "plt.title('Distribution of means of all columns');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d205e01b009224a3189903e1858dd592fb222d2d"
   },
   "outputs": [],
   "source": [
    "# we have no missing values\n",
    "train.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30e64cca712542d662201263914d8fc25496563e"
   },
   "outputs": [],
   "source": [
    "print('Distributions of first 28 columns')\n",
    "plt.figure(figsize=(26, 24))\n",
    "for i, col in enumerate(list(train.columns)[2:30]):\n",
    "    plt.subplot(7, 4, i + 1)\n",
    "    plt.hist(train[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "227daacd39977e5658c7e27db2686d8f65fdff3c"
   },
   "outputs": [],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89e9ed49ceff33d27cd1888336c3c46a38c5c8aa"
   },
   "source": [
    "From this overview we can see the following things:\n",
    "* target is binary and has some disbalance: 36% of samples belong to 0 class;\n",
    "* values in columns are more or less similar;\n",
    "* columns have std of 1 +/- 0.1 (min and max values are 0.889, 1.117 respectively);\n",
    "* columns have mean of 0 +/- 0.15 (min and max values are -0.2, 0.1896 respectively);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06df27b43428261da7daf02e708b934519d78ac2"
   },
   "source": [
    "Let's have a look at correlations now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae63462aa70238f0a2858de687dc7d2ae319589a"
   },
   "outputs": [],
   "source": [
    "corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "corrs = corrs[corrs['level_0'] != corrs['level_1']]\n",
    "corrs.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2d921a5d3bf606b88853988c10acad020685334"
   },
   "source": [
    "We can see that correlations between features are lower that 0.3 and the most correlated feature with target has correlation of 0.37. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4f28e1e3c847e2fe165034dd870154afb7fe939"
   },
   "source": [
    "<a id=\"bm\"></a>\n",
    "## Basic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f3eef02d6beac1b76f88c75bb842da9a313f592"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "n_fold = 20\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "32b8fe75f240c11df7eaf3ed91b76d9260f999c9"
   },
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds=folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        # print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "            \n",
    "            model = lgb.train(params,\n",
    "                    train_data,\n",
    "                    num_boost_round=2000,\n",
    "                    valid_sets = [train_data, valid_data],\n",
    "                    verbose_eval=500,\n",
    "                    early_stopping_rounds = 200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=20000,  eval_metric='AUC', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = roc_auc_score(y_valid, y_pred_valid)\n",
    "            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n",
    "            # print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "        if model_type == 'glm':\n",
    "            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "            model_results = model.fit()\n",
    "            model_results.predict(X_test)\n",
    "            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n",
    "            score = roc_auc_score(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model_results.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(roc_auc_score(y_valid, y_pred_valid))\n",
    "\n",
    "        if averaging == 'usual':\n",
    "            prediction += y_pred\n",
    "        elif averaging == 'rank':\n",
    "            prediction += pd.Series(y_pred).rank().values  \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction, scores\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe9017199e72183e9686e55a3608c9339b779302",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A lot of people are using logreg currently, let's try\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54c06c0b3a1cb1f343c96dee67d3da28afd734a1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A lot of people are using logreg currently, let's try\n",
    "cat_params = {'learning_rate': 0.02,\n",
    "              'depth': 5,\n",
    "              'l2_leaf_reg': 10,\n",
    "              'bootstrap_type': 'Bernoulli',\n",
    "              #'metric_period': 500,\n",
    "              'od_type': 'Iter',\n",
    "              'od_wait': 50,\n",
    "              'random_seed': 11,\n",
    "              'allow_writing_files': False}\n",
    "oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=cat_params, model_type='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c3627bba0ddce8c7eba677b04ce2075e39a9e20"
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=400,  eval_metric='AUC', **cat_params)\n",
    "model.fit(X_train, y_train, cat_features=[], use_best_model=True, verbose=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b482e006b077109f725547d616818eb92107d23"
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr, prediction_lr_repeated, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a77af2f9fa7008564e3686c008a8fb21e41e5ce2"
   },
   "source": [
    "Submitting `prediction_lr` gives 0.847 on leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "600682b545014ae67e19a8b04724e75767be6014"
   },
   "source": [
    "<a id=\"eli5\"></a>\n",
    "## ELI5\n",
    "\n",
    "ELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(model, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f16f5fee606cb48d35c0cb95e123c7542aacac28"
   },
   "outputs": [],
   "source": [
    "(model.coef_ != 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c738de31f86152ced6cb35ddb8d3569e7b49a6e"
   },
   "source": [
    "We can see that There are several features with highly positive weights and more features with negative weights. In fact there are only 32 features, which are important according to ELI5. It is worth noticing though, that the model itself had 34 non-zero features, so ELI5 dropped only 2 features.. Let's try using only them for the submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c78622313d2963bb28aa2870e0ed9f811f315ba"
   },
   "outputs": [],
   "source": [
    "top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\n",
    "X_train = train[top_features]\n",
    "X_test = test[top_features]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86e7050847ae7d7462e728d97922a1072c013765",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0eebf500fd8e7cfbb9ea41bfc35cc500da31e542"
   },
   "source": [
    "Wow, we got improvement from 0.7226 to 0.7486 on CV! But this submission gives 0.845 on leaderboard. So it decreases score slightly. Let's try other things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fbee2b85bae60cf0607b977692306eb380582e5c"
   },
   "source": [
    "<a id=\"eli5p\"></a>\n",
    "### Permutation importance\n",
    "There is also another way of using eli5 - we could have a look at permutation importance. It works in the following way:\n",
    "* We fit a model;\n",
    "* We randomly shuffle one column of validation data and calculate the score;\n",
    "* If the score dropped significantly, it means that the feature is important;\n",
    "\n",
    "You can read more about this approach here: https://www.kaggle.com/dansbecker/permutation-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "242439bf4036359fc07864ac41d0bccac6f1d9c6"
   },
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "eli5.show_weights(perm, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26dd053929c39014a3244d8d2471e8b2cdb5ac0f"
   },
   "outputs": [],
   "source": [
    "top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(perm).feature if 'BIAS' not in i]\n",
    "X_train = train[top_features]\n",
    "X_test = test[top_features]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcbc3e88b57bab0d1220c41d222540d42a445622"
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr1, prediction_lr1, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ba3da8d6a72868c99fd03de89c4f636d48836d7"
   },
   "source": [
    "Wow, if we select columns by permutation importance, CV score drops significantly. It seems it doesn't work well in out case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a010cfbba09f2846f05211678657f87614c219c4"
   },
   "source": [
    "<a id=\"shap\"></a>\n",
    "## SHAP\n",
    "\n",
    "Another interesting tool is SHAP. It also provides explanations for a variety of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "5869cb071f42e20e9e420c0a9d1ef584dd2b2417"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33c4481d2b911208738c7ffc7042259108bb5736",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explainer = shap.LinearExplainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1aa2616547304144c41566d74c17e922325ba64"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd6ac9eda7175a382b6b0adf4e5fe95ecd02e553"
   },
   "source": [
    "It could be difficult to interpret this plot when you see it for the first time. It shows how features impact predictions. For example for feature 33 low values have a negative impact on model predictions (zero is more likely), and high values have a positive impace (ones are more likely). Feature 217 has an opposite effect: low values have a positive impact and high values have a negative impact.\n",
    "\n",
    "But we will need to select features manually... let's use a library for that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb5994704ef34d093e24d4a3983de73b6c98c1bd"
   },
   "source": [
    "<a id=\"mlextend\"></a>\n",
    "## Mlextend SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46b0caca758e3864e8820e8e21af53a103890444",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfs1 = SFS(model, \n",
    "           k_features=(10, 15), \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=0,\n",
    "           scoring='roc_auc',\n",
    "           cv=folds,\n",
    "          n_jobs=-1)\n",
    "\n",
    "sfs1 = sfs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28fc74c03bd5502f575d2121624737d78f626cfb"
   },
   "outputs": [],
   "source": [
    "fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "plt.ylim([0.8, 1])\n",
    "plt.title('Sequential Forward Selection (w. StdDev)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1f69c3eec7c039557ba893e3572f3c95090044b"
   },
   "outputs": [],
   "source": [
    "top_features = list(sfs1.k_feature_names_)\n",
    "X_train = train[top_features]\n",
    "X_test = test[top_features]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcfda893ab182e944fa333b8540021c4a32b9abe"
   },
   "source": [
    "And this gives 0.811 on leaderboard. Overfitting! It seems that feature selection isn't the best approach. Let's try building various models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ebbe316febc785339dde063c927c297d124ef72"
   },
   "source": [
    "<a id=\"model\"></a>\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "af8a61dc9fc0d142b234374439c9666972d7a32b"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afce168ca97864abaac48bbb07820edfc89e7b2b"
   },
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "\n",
    "parameter_grid = {'class_weight' : ['balanced', None],\n",
    "                  'penalty' : ['l2'],\n",
    "                  'C' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "                  'solver': ['newton-cg', 'sag', 'lbfgs']\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e09d949518bb82ecfa07e8b0a87dce72f286ade"
   },
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegression(solver='liblinear', max_iter=10000)\n",
    "\n",
    "parameter_grid = {'class_weight' : ['balanced', None],\n",
    "                  'penalty' : ['l2', 'l1'],\n",
    "                  'C' : [0.001, 0.01, 0.08, 0.1, 0.15, 1.0, 10.0, 100.0],\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d93e5cda1dd01559d79eca69516240b2f609e3a"
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ade6c06755d1664f1ce194bdb7325ea9f7bb94c"
   },
   "source": [
    "So, parameters for logreg are optimal, let's try other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f035ad28718fe9f932b4ea63637a3ac3c1bd9d1"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "oof_gnb, prediction_gnb, scores_gnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d00ff4a78c464d35a25c101c4f2f03efc453c615"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier()\n",
    "\n",
    "parameter_grid = {'n_estimators': [5, 10, 20, 50, 100],\n",
    "                  'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(abc, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcb3555d31230c73b1ab65f5371455a0bdd68999",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(**grid_search.best_params_)\n",
    "oof_abc, prediction_abc, scores_abc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f57a2c0f0da71197a1163a8bed02efc0c8d89e3b"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier()\n",
    "\n",
    "parameter_grid = {'n_estimators': [10, 50, 100, 1000],\n",
    "                  'max_depth': [None, 3, 5, 15]\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(etc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "etc = ExtraTreesClassifier(**grid_search.best_params_)\n",
    "oof_etc, prediction_etc, scores_etc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a529bdac93c14d7cf4430037016784c5f913b8c"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "parameter_grid = {'n_estimators': [10, 50, 100, 1000],\n",
    "                  'max_depth': [None, 3, 5, 15]\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(rfc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "rfc = RandomForestClassifier(**grid_search.best_params_)\n",
    "oof_rfc, prediction_rfc, scores_rfc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd36b69df66bb55c2506eb40bf42fd64698d583f"
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "gpc = GaussianProcessClassifier()\n",
    "oof_gpc, prediction_gpc, scores_gpc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67f9ca1934727e6cbe6434d7d911e9134acf6556"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(probability=True, gamma='scale')\n",
    "\n",
    "parameter_grid = {'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "                  'kernel': ['linear', 'poly', 'rbf'],\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "svc = SVC(probability=True, gamma='scale', **grid_search.best_params_)\n",
    "oof_svc, prediction_svc, scores_svc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e6940fd73f9064824626c17e0abd117824bf261"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "parameter_grid = {'n_neighbors': [2, 3, 5, 10, 20],\n",
    "                  'weights': ['uniform', 'distance'],\n",
    "                  'leaf_size': [5, 10, 30]\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(knc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "knc = KNeighborsClassifier(**grid_search.best_params_)\n",
    "oof_knc, prediction_knc, scores_knc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=knc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad1c850d35de23d06d5c267b5c3082188d1f9999"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "parameter_grid = {'alpha': [0.0001, 1, 2, 10]\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(bnb, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "bnb = BernoulliNB(**grid_search.best_params_)\n",
    "oof_bnb, prediction_bnb, scores_bnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54452c184551d9d2ad94f4dc468f92e75cc3f984"
   },
   "outputs": [],
   "source": [
    "sgd = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001)\n",
    "\n",
    "parameter_grid = {'loss': ['log', 'modified_huber'],\n",
    "                  'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                  'alpha': [0.001, 0.01],\n",
    "                  'l1_ratio': [0, 0.15, 0.5, 1.0],\n",
    "                  'learning_rate': ['optimal', 'invscaling', 'adaptive']\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(sgd, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "sgd = linear_model.SGDClassifier(eta0=1, tol=0.0001, **grid_search.best_params_)\n",
    "oof_sgd, prediction_sgd, scores_sgd = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2335b870061080430fc25c9e99111357088824b8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame({'LogisticRegression': scores})\n",
    "scores_df['GaussianNB'] = scores_gnb\n",
    "scores_df['AdaBoostClassifier'] = scores_abc\n",
    "scores_df['ExtraTreesClassifier'] = scores_etc\n",
    "scores_df['GaussianProcessClassifier'] = scores_gpc\n",
    "scores_df['SVC'] = scores_svc\n",
    "scores_df['KNeighborsClassifier'] = scores_knc\n",
    "scores_df['BernoulliNB'] = scores_bnb\n",
    "scores_df['SGDClassifier'] = scores_sgd\n",
    "scores_df['RandomForestClassifier'] = scores_rfc\n",
    "\n",
    "sns.boxplot(data=scores_df);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f81c4775a537def6405793464109d7e80f32697"
   },
   "source": [
    "We can see that logistic regression is superior to most other models. Only SVC is comparable. It seems that other models either overfit or can't work on this small dataset.\n",
    "\n",
    "\n",
    "Let's try submitting a blend of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "eb67be43e33c0af7539ad43e47f73c7a261e0aea"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "596ac8130214fb6a70d1f1bfa12215fdca4421c5"
   },
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('../input/sample_submission.csv')\n",
    "# submission['target'] = (prediction_lr + prediction_svc) / 2\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0b1fd02763ea153770d4c8c496708719b85528a"
   },
   "outputs": [],
   "source": [
    "plt.hist(prediction_lr, label='logreg');\n",
    "plt.hist(prediction_svc, label='svc');\n",
    "plt.hist((prediction_lr + prediction_svc) / 2, label='blend');\n",
    "plt.title('Distribution of out of fold predictions');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27fe8ff687b97b8b77e00c2674d73af7dc0c8870"
   },
   "source": [
    "Sadly blend gives 0.831 on LB. Again no luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3fab09097febdb8a9818c7d3f040aae6fe853484"
   },
   "source": [
    "Let's try generating some features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac1ba90583579cbe3ad953280b7f8134f218dbc8"
   },
   "source": [
    "<a id=\"poly\"></a>\n",
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e0113dcf9a8fe6a4c88fa1d03dbedb0412440e3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eed696b6f14c7fe3c0012994073ff50f4b753045"
   },
   "source": [
    "The number of polynomial features is ~45k which is too much. We need some way to select some of them. Let's try use correlations with target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d0527be4dbc026aa059d1da2fed0b4b25eda269"
   },
   "outputs": [],
   "source": [
    "cor = pd.DataFrame(X_train_poly).corrwith(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b9691673c8833ee652567e7d4c09c675a6430a76",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = []\n",
    "for i in range(10, 510, 5):\n",
    "    top_corr_cols = list(cor.abs().sort_values().tail(i).reset_index()['index'].values)\n",
    "    X_train_poly1 = X_train_poly[:, top_corr_cols]\n",
    "    X_test_poly1 = X_test_poly[:, top_corr_cols]\n",
    "    oof_lr_poly, prediction_lr_poly, scores = train_model(X_train_poly1, X_test_poly1, y_train, params=None, model_type='sklearn', model=model)\n",
    "    sc.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "cf28344a6d0b663d2ba57724bc526611096ebf01"
   },
   "outputs": [],
   "source": [
    "data = [go.Scatter(\n",
    "        x = list(range(10, 510, 5)),\n",
    "        y = [np.round(np.mean(i), 4) for i in sc],\n",
    "        name = 'CV scores'\n",
    "    )]\n",
    "layout = go.Layout(dict(title = \"Top N poly features vs CV\",\n",
    "                  xaxis = dict(title = 'Top N features'),\n",
    "                  yaxis = dict(title = 'CV score'),\n",
    "                  ))\n",
    "py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18e458397141b8269fd4bd711ab5bbc540face79"
   },
   "source": [
    "Not suprisingly we overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b80ab00ed812cfb5509c200cf17825efffc83f60"
   },
   "outputs": [],
   "source": [
    "top_corr_cols = list(cor.abs().sort_values().tail(300).reset_index()['index'].values)\n",
    "X_train_poly1 = X_train_poly[:, top_corr_cols]\n",
    "X_test_poly1 = X_test_poly[:, top_corr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26f39c9dc3fd3ae4b11cf0501c1fab1399bc6fc8"
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr_poly, prediction_lr_poly, scores = train_model(X_train_poly1, X_test_poly1, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11c41e8fb3cc63aa6e13528e13da298cf9557713"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = prediction_lr_poly\n",
    "# submission.to_csv('submission_poly.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9aeaf79451f9b2bc68851795b6abadff4fe7351"
   },
   "source": [
    "Score became much lower. So this is also a bad idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "03af3b65d3f687689051ac93662029c4e2f58600"
   },
   "source": [
    "<a id=\"stats\"></a>\n",
    "## Adding statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a197ee822f72ac1afbdb619e795195300a1b47c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "X_train['300'] = X_train.std(1)\n",
    "X_test['300'] = X_test.std(1)\n",
    "scaler = StandardScaler()\n",
    "X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr_1, prediction_lr_1, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1afb310da8f11a8fcc5fdc4a6a3836176fd3ac98"
   },
   "source": [
    "Let's compare with repeated KFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57dd2ffc7c366e301c77145eba46c62ed6d114bd"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "X_train['300'] = X_train.std(1)\n",
    "X_test['300'] = X_test.std(1)\n",
    "scaler = StandardScaler()\n",
    "X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr_1, prediction_lr_1_repeated, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = prediction_lr_1_repeated\n",
    "submission.to_csv('repeated_fold_features.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa509eaef0a1e7152f00103d1d7e02775893aa2e"
   },
   "source": [
    "CV increased a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "690f4e5f6cf2bdb293f6fca2c3c4e1e570670733"
   },
   "source": [
    "<a id=\"dist\"></a>\n",
    "## Adding distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "938d99d96431cbba45756bbe4cc2ff09184def66"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "main_cols = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24bebe8ebc23129858024eb89f2e54868e0cc689",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(5, n_jobs=-1)\n",
    "neigh.fit(X_train)\n",
    "\n",
    "dists, _ = neigh.kneighbors(X_train, n_neighbors=5)\n",
    "mean_dist = dists.mean(axis=1)\n",
    "max_dist = dists.max(axis=1)\n",
    "min_dist = dists.min(axis=1)\n",
    "\n",
    "X_train['300'] = X_train.std(1)\n",
    "X_train = np.hstack((X_train, mean_dist.reshape(-1, 1), max_dist.reshape(-1, 1), min_dist.reshape(-1, 1)))\n",
    "\n",
    "test_dists, _ = neigh.kneighbors(X_test, n_neighbors=5)\n",
    "\n",
    "test_mean_dist = test_dists.mean(axis=1)\n",
    "test_max_dist = test_dists.max(axis=1)\n",
    "test_min_dist = test_dists.min(axis=1)\n",
    "\n",
    "X_test['300'] = X_test.std(1)\n",
    "X_test = np.hstack((X_test, test_mean_dist.reshape(-1, 1), test_max_dist.reshape(-1, 1), test_min_dist.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d96c7e49b6e4048f200dd18e4975466b983e3fd3"
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr_2, prediction_lr_2, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\n",
    "submission['target'] = prediction_lr_2\n",
    "submission.to_csv('nn_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ec33fc61e002ff5f963fc3c71c9d3da5f82d7ef"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = (prediction_lr_1 + prediction_lr_2) / 2\n",
    "# submission.to_csv('blend.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "191f51d36b953c8b555bd12edd6a298bbb2a193e"
   },
   "source": [
    "<a id=\"select\"></a>\n",
    "## Sklearn feature selection\n",
    "\n",
    "Sklearn has several methods to do feature selection. Let's try some of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb4551a447ecefb4f2ff7aba6d0b3cdc5abee197",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# baseline score\n",
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n",
    "oof_lr_1, prediction_lr_1, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "6d268e80c10bea5245806741fd18e8534cfdb9c0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_dict = {'f_classif': [], 'mutual_info_classif': []}\n",
    "for i in range(5, 100, 5):\n",
    "    s1 = SelectPercentile(f_classif, percentile=i)\n",
    "    X_train1 = s1.fit_transform(X_train, y_train.values.astype(int))\n",
    "    X_test1 = s1.transform(X_test)\n",
    "    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_dict['f_classif'].append(np.mean(scores))\n",
    "    \n",
    "    s2 = SelectPercentile(mutual_info_classif, percentile=i)\n",
    "    X_train1 = s2.fit_transform(X_train, y_train.values.astype(int))\n",
    "    X_test1 = s2.transform(X_test)\n",
    "    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_dict['mutual_info_classif'].append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "dfc7618d9ed474ee4569aa79b0cdc9c98efa471e"
   },
   "outputs": [],
   "source": [
    "data = [go.Scatter(\n",
    "        x = list(range(5, 100, 5)),\n",
    "        y = scores_dict['f_classif'],\n",
    "        name = 'CV scores f_classif'\n",
    "    ), go.Scatter(\n",
    "        x = list(range(5, 100, 5)),\n",
    "        y = scores_dict['mutual_info_classif'],\n",
    "        name = 'CV scores mutual_info_classif')]\n",
    "layout = go.Layout(dict(title = \"Top N features by percentile vs CV\",\n",
    "                  xaxis = dict(title = 'Top N features by percentile'),\n",
    "                  yaxis = dict(title = 'CV score'),\n",
    "                  ))\n",
    "py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "65f03bc1fe3bd975d0ec4ad5bf782a4e8a04c444",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_dict = {'f_classif': [], 'mutual_info_classif': []}\n",
    "for i in range(10, 301, 10):\n",
    "    s1 = SelectKBest(f_classif, k=i)\n",
    "    X_train1 = s1.fit_transform(X_train, y_train.values.astype(int))\n",
    "    X_test1 = s1.transform(X_test)\n",
    "    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_dict['f_classif'].append(np.mean(scores))\n",
    "    \n",
    "    s2 = SelectKBest(mutual_info_classif, k=i)\n",
    "    X_train1 = s2.fit_transform(X_train, y_train.values.astype(int))\n",
    "    X_test1 = s2.transform(X_test)\n",
    "    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_dict['mutual_info_classif'].append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4c3f3c18f6d3fd4b537876e1a2ddc068f755845d"
   },
   "outputs": [],
   "source": [
    "data = [go.Scatter(\n",
    "        x = list(range(10, 301, 10)),\n",
    "        y = scores_dict['f_classif'],\n",
    "        name = 'CV scores f_classif'\n",
    "    ), go.Scatter(\n",
    "        x = list(range(10, 301, 10)),\n",
    "        y = scores_dict['mutual_info_classif'],\n",
    "        name = 'CV scores mutual_info_classif')]\n",
    "layout = go.Layout(dict(title = \"Top N features by SelectKBest vs CV\",\n",
    "                  xaxis = dict(title = 'Top N features by SelectKBest'),\n",
    "                  yaxis = dict(title = 'CV score'),\n",
    "                  ))\n",
    "py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae28a13e61b4a49b3450aab6fc9e0643385e4790"
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=60)\n",
    "X_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\n",
    "X_testK = selector.transform(X_test)\n",
    "oof_lr_1, prediction_lr_1, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "610aac831f9d5b4e419729f7655f947cbd4a5f71"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = prediction_lr_1\n",
    "# submission.to_csv('top_n_features.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "c6da95040189d16d4dea04902378af6a4a3a2049",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_list = []\n",
    "for i in range(10, 301, 5):\n",
    "    s = RFE(model, i, step=1)\n",
    "    X_train1 = s.fit_transform(X_train, y_train.values.astype(int))\n",
    "    X_test1 = s.transform(X_test)\n",
    "    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_list.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ce0ea15ac66bfc710d77430137a652b73e55368c"
   },
   "outputs": [],
   "source": [
    "data = [go.Scatter(\n",
    "        x = list(range(10, 301, 5)),\n",
    "        y = scores_list,\n",
    "        name = 'CV scores RFE'\n",
    "    )]\n",
    "layout = go.Layout(dict(title = \"Top N features by RFE vs CV\",\n",
    "                  xaxis = dict(title = 'Top N features by RFE'),\n",
    "                  yaxis = dict(title = 'CV score'),\n",
    "                  ))\n",
    "py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae28a13e61b4a49b3450aab6fc9e0643385e4790"
   },
   "outputs": [],
   "source": [
    "selector = RFE(model, 20, step=1)\n",
    "X_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\n",
    "X_testK = selector.transform(X_test)\n",
    "oof_lr_1, prediction_lr_rfe_20, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "610aac831f9d5b4e419729f7655f947cbd4a5f71"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = prediction_lr_rfe_20\n",
    "submission.to_csv('rfe_20.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f09566d8b120c453df287f6e0ffc3ec4ce02f34"
   },
   "source": [
    "<a id=\"dglm\"></a>\n",
    "## GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22697edeca913a6fc486b11f7348eeae43d723b1"
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=15)\n",
    "X_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\n",
    "X_testK = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b76fa19188b75a5db679f026bf01ee6d438891cc"
   },
   "outputs": [],
   "source": [
    "oof_glm, prediction_glm, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='glm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e1455a96499a4ef25fb84cccb6a9c11f152feaf0"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = prediction_glm\n",
    "submission.to_csv('glm.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d1482994a31ac493f4f5889e52d547027d32f7c"
   },
   "source": [
    "<a id=\"selected\"></a>\n",
    "## Selected top_features + statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5da819651ae126239b1c5349ef290c2ddc37e188",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5_weights = eli5.formatters.as_dataframe.explain_weights_df(model)\n",
    "eli5_weights['weight'] = eli5_weights['weight'].abs()\n",
    "eli5_weights = eli5_weights.sort_values('weight', ascending=False)\n",
    "eli5_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "f99e065338d8cd78ca739b21d1ea4a26d577d379"
   },
   "outputs": [],
   "source": [
    "train['mean'] = train.mean(1)\n",
    "train['std'] = train.std(1)\n",
    "test['mean'] = test.mean(1)\n",
    "test['std'] = test.std(1)\n",
    "\n",
    "scores_dict = {'simple': [], 'with_std': [], 'with_mean': []}\n",
    "for i in range(1, eli5_weights.shape[0] + 1):\n",
    "    top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:i]\n",
    "    \n",
    "    X_train = train[top_features]\n",
    "    X_test = test[top_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_dict['simple'].append(np.mean(scores))\n",
    "    \n",
    "    X_train = train[top_features + ['mean']]\n",
    "    X_test = test[top_features + ['mean']]\n",
    "    scaler = StandardScaler()\n",
    "    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_dict['with_mean'].append(np.mean(scores))\n",
    "    \n",
    "    X_train = train[top_features + ['std']]\n",
    "    X_test = test[top_features + ['std']]\n",
    "    scaler = StandardScaler()\n",
    "    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model)\n",
    "    scores_dict['with_std'].append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "766695d8f41ba8a090c7a4de39d91fdcb0fe24e4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [go.Scatter(\n",
    "        x = list(range(1, eli5_weights.shape[0] + 1)),\n",
    "        y = scores_dict['simple'],\n",
    "        name = 'Simple CV scores'\n",
    "    ), go.Scatter(\n",
    "        x = list(range(1, eli5_weights.shape[0] + 1)),\n",
    "        y = scores_dict['with_mean'],\n",
    "        name = 'With mean CV scores'\n",
    "    ), go.Scatter(\n",
    "        x = list(range(1, eli5_weights.shape[0] + 1)),\n",
    "        y = scores_dict['with_std'],\n",
    "        name = 'With std CV scores'\n",
    "    )]\n",
    "layout = go.Layout(dict(title = \"Top N features vs CV\",\n",
    "                  xaxis = dict(title = 'Top N features'),\n",
    "                  yaxis = dict(title = 'CV score'),\n",
    "                  ))\n",
    "py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "1d73a5779b4fa314098255a8d5e7b83cbd5050c7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['mean'] = train.mean(1)\n",
    "train['std'] = train.std(1)\n",
    "test['mean'] = test.mean(1)\n",
    "test['std'] = test.std(1)\n",
    "\n",
    "scores_dict = {'simple': [], 'with_std': [], 'with_mean': []}\n",
    "for i in range(1, eli5_weights.shape[0] + 1):\n",
    "    top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:i]\n",
    "    \n",
    "    X_train = train[top_features]\n",
    "    X_test = test[top_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n",
    "    scores_dict['simple'].append(np.mean(scores))\n",
    "    \n",
    "    X_train = train[top_features + ['mean']]\n",
    "    X_test = test[top_features + ['mean']]\n",
    "    scaler = StandardScaler()\n",
    "    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n",
    "    scores_dict['with_mean'].append(np.mean(scores))\n",
    "    \n",
    "    X_train = train[top_features + ['std']]\n",
    "    X_test = test[top_features + ['std']]\n",
    "    scaler = StandardScaler()\n",
    "    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n",
    "    scores_dict['with_std'].append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e8b60145a9e793fba0ef45cf9b3af18a7339ea92"
   },
   "outputs": [],
   "source": [
    "data = [go.Scatter(\n",
    "        x = list(range(1, eli5_weights.shape[0] + 1)),\n",
    "        y = scores_dict['simple'],\n",
    "        name = 'Simple CV scores'\n",
    "    ), go.Scatter(\n",
    "        x = list(range(1, eli5_weights.shape[0] + 1)),\n",
    "        y = scores_dict['with_mean'],\n",
    "        name = 'With mean CV scores'\n",
    "    ), go.Scatter(\n",
    "        x = list(range(1, eli5_weights.shape[0] + 1)),\n",
    "        y = scores_dict['with_std'],\n",
    "        name = 'With std CV scores'\n",
    "    )]\n",
    "layout = go.Layout(dict(title = \"Top N features vs repeated folds CV\",\n",
    "                  xaxis = dict(title = 'Top N features'),\n",
    "                  yaxis = dict(title = 'CV score'),\n",
    "                  ))\n",
    "py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "339d5901af5e8403987d343043661d83d58ab87f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:8]\n",
    "\n",
    "X_train = train[top_features + ['mean']]\n",
    "X_test = test[top_features + ['mean']]\n",
    "scaler = StandardScaler()\n",
    "X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n",
    "\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = prediction_lr\n",
    "submission.to_csv('submission_top8.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a40abaf1008de69645abc9aad68f65eaef94ef9b"
   },
   "outputs": [],
   "source": [
    "top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:10]\n",
    "\n",
    "X_train = train[top_features + ['mean']]\n",
    "X_test = test[top_features + ['mean']]\n",
    "scaler = StandardScaler()\n",
    "X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n",
    "X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n",
    "oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n",
    "\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['target'] = prediction_lr\n",
    "submission.to_csv('submission_top10.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
